

<!doctype html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>AIMET Model Quantization &#8212; AI Model Efficiency Toolkit Documentation: ver tf-torch-cpu_1.24.0</title>
    <link rel="stylesheet" href="../_static/bizstyle.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "tex2jax_ignore|mathjax_ignore|document", "processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script type="text/javascript" src="../_static/bizstyle.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="AIMET Model Compression" href="model_compression.html" />
    <link rel="prev" title="AI Model Efficiency Toolkit User Guide" href="index.html" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <!--[if lt IE 9]>
    <script type="text/javascript" src="_static/css3-mediaqueries.js"></script>
    <![endif]-->
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="model_compression.html" title="AIMET Model Compression"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="index.html" title="AI Model Efficiency Toolkit User Guide"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">AI Model Efficiency Toolkit Documentation: ver tf-torch-cpu_1.24.0</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="../_static/brain_logo.png" alt="Logo"/>
            </a></p>
  <h3><a href="index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">AIMET Model Quantization</a><ul>
<li><a class="reference internal" href="#use-cases">Use Cases</a></li>
<li><a class="reference internal" href="#aimet-quantization-features">AIMET Quantization Features</a></li>
<li><a class="reference internal" href="#aimet-quantization-workflow">AIMET Quantization Workflow</a></li>
<li><a class="reference internal" href="#debugging-guidelines">Debugging Guidelines</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="index.html"
                        title="previous chapter">AI Model Efficiency Toolkit User Guide</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="model_compression.html"
                        title="next chapter">AIMET Model Compression</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="aimet-model-quantization">
<span id="ug-model-quantization"></span><h1>AIMET Model Quantization<a class="headerlink" href="#aimet-model-quantization" title="Permalink to this headline">¶</a></h1>
<p>Models are generally trained on floating-point hardware like CPUs and GPUs. However, when these trained models are run
on quantized hardware that support fixed-precision operations, model parameters are converted from floating-point
precision to fixed precision. As an example, when running on hardware that supports 8-bit integer operations, the
floating point parameters in the trained model need to be converted to 8-bit integers. It is observed that for some
models, running on an 8-bit fixed-precision runtime introduces a loss in accuracy due to noise added from the use
of fixed precision parameters and fixed precision operations.</p>
<p>AIMET provides multiple techniques and tools which help to create quantized models with a minimal loss in accuracy
relative to floating-point models.</p>
<p>This section provides information on typical use cases and AIMET’s quantization features.</p>
<div class="section" id="use-cases">
<h2>Use Cases<a class="headerlink" href="#use-cases" title="Permalink to this headline">¶</a></h2>
<p>1. <strong>Predict on-target accuracy</strong>: AIMET enables a user to simulate the effects of quantization to get a first order
estimate of the model’s accuracy when run on quantized targets. This is useful to get an estimate of on-target accuracy
without needing an actual target platform. Note that to create a simulation model, AIMET uses representative data
samples to compute per-layer quantization encodings.</p>
<blockquote>
<div><img alt="../_images/quant_use_case_1.PNG" src="../_images/quant_use_case_1.PNG" />
</div></blockquote>
<p>2. <strong>Post-Training Quantization (PTQ)</strong>: PTQ techniques attempt to make a model more quantization friendly without
requiring model re-training/fine-tuning. PTQ (as opposed to fine-tuning) is recommended as a first step in a
quantization workflow due to the following advantages:</p>
<ul>
<li><p>No need for the original training pipeline; an evaluation pipeline is sufficient</p></li>
<li><p>Only requires a small unlabeled dataset for calibration (can even be data-free in some scenarios)</p></li>
<li><p>Fast, simple, and easy to use</p>
<blockquote>
<div><img alt="../_images/quant_use_case_3.PNG" src="../_images/quant_use_case_3.PNG" />
</div></blockquote>
</li>
</ul>
<p>Note that with PTQ techniques, the quantized model accuracy may still have a gap relative to the floating-point model.
In such a scenario, or to even further improve the model accuracy, fine-tuning is recommended.</p>
<p>3. <strong>Quantization-Aware Training (QAT)/Fine-Tuning</strong>: QAT enables a user to fine-tune a model with quantization
operations inserted in network graph, which in effect adapts the model parameters to be robust to quantization noise.
While QAT requires access to a training pipeline and dataset, and takes longer to run due to needing a few epochs of
fine-tuning, it can provide better accuracy especially at low bitwidths. A typical QAT workflow is illustrated below.</p>
<blockquote>
<div><img alt="../_images/quant_use_case_2.PNG" src="../_images/quant_use_case_2.PNG" />
</div></blockquote>
</div>
<div class="section" id="aimet-quantization-features">
<h2>AIMET Quantization Features<a class="headerlink" href="#aimet-quantization-features" title="Permalink to this headline">¶</a></h2>
<ul>
<li><dl class="simple">
<dt><a class="reference internal" href="quantization_sim.html"><span class="doc">Quantization Simulation</span></a>:</dt><dd><p>QuantSim enables a user to modify a model by adding quantization simulation ops. When an evaluation is run on a
model with these quantization simulation ops, the user can observe a first-order simulation of expected accuracy on
quantized hardware.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Post-Training Quantization (PTQ) Techniques</dt><dd><p>Post-training quantization techniques help a model improve quantized accuracy without needing to re-train.</p>
<ul class="simple">
<li><dl class="simple">
<dt><a class="reference internal" href="auto_quant.html#ug-auto-quant"><span class="std std-ref">AutoQuant</span></a>:</dt><dd><p>AIMET provies an API that integrates the post-training quantization techniques described below. AutoQuant is
recommended for PTQ. If desired, individual techniques can be invoked using standalone feature specific APIs.</p>
<ul>
<li><dl class="simple">
<dt><a class="reference internal" href="adaround.html#ug-adaround"><span class="std std-ref">Adaptive Rounding (AdaRound)</span></a>:</dt><dd><p>Determines optimal rounding for weight tensors to improve quantized performance.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference internal" href="post_training_quant_techniques.html#ug-post-training-quantization"><span class="std std-ref">Cross-Layer Equalization</span></a>:</dt><dd><p>Equalizes weight ranges in consecutive layers.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference internal" href="post_training_quant_techniques.html#ug-post-training-quantization"><span class="std std-ref">Bias Correction</span></a> [Deprecated]:</dt><dd><p>Bias Correction is considered deprecated. It is advised to use AdaRound instead.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl>
<dt><a class="reference internal" href="quantization_aware_training.html#ug-quantization-aware-training"><span class="std std-ref">Quantization-Aware Training (QAT)</span></a>:</dt><dd><p>QAT allows users to take a QuantSim model and further fine-tune the model parameters by taking quantization into
account.</p>
<p>Two modes of QAT are supported:</p>
<ul class="simple">
<li><dl class="simple">
<dt>Regular QAT:</dt><dd><p>Fine-tuning of model parameters. Trainable parameters such as module weights, biases, etc. can be
updated. The scale and offset quantization parameters for activation quantizers remain constant. Scale and
offset parameters for weight quantizers will update to reflect new weight values after each training step.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>QAT with Range Learning:</dt><dd><p>In addition to trainable module weights and scale/offset parameters for weight quantizers, scale/offset
parameters for activation quantizers are also updated during each training step.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Debugging/Analysis Tools</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><a class="reference internal" href="quant_analyzer.html#ug-quant-analyzer"><span class="std std-ref">QuantAnalyzer</span></a>:</dt><dd><p>Automated debugging of the model to understand sensitivity to weight and/or activation quantization, individual
layer sensitivity, etc.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><a class="reference internal" href="visualization_quant.html#ug-quantization-visualization"><span class="std std-ref">Visualizations</span></a>:</dt><dd><p>Visualizations and histograms of weight and activation ranges.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="aimet-quantization-workflow">
<h2>AIMET Quantization Workflow<a class="headerlink" href="#aimet-quantization-workflow" title="Permalink to this headline">¶</a></h2>
<p>This section describes the recommended workflow for quantizing a neural network.</p>
<blockquote>
<div><img alt="../_images/quantization_workflow.PNG" src="../_images/quantization_workflow.PNG" />
</div></blockquote>
<p><strong>1. Model prep and validation</strong></p>
<p>Before attempting quantization, ensure that models have been defined in accordance to model guidelines. These guidelines
depend on the ML framework the model is written in.</p>
<dl>
<dt>Pytorch:</dt><dd><p><a class="reference internal" href="../api_docs/torch_model_guidelines.html"><span class="doc">PyTorch Model Guidelines</span></a></p>
<p>In the case of PyTorch, there exist the Model Validator utility, to automate the checking of certain PyTorch model
requirements, as well as the Model Preparer utility, to automate the updating of the model definition to align with
certain requirements.</p>
<p>In this model prep and validation phase, we advise the following flow:</p>
<img alt="../_images/pytorch_model_prep_and_validate.PNG" src="../_images/pytorch_model_prep_and_validate.PNG" />
<p>Users can use the model validator utility first to check if the model can be run with AIMET. If validator checks
fail, users can first try using model preparer in their pipeline, an automated feature for updating models, and
retry the model validator to see if checks now pass. If the validator continues to print warnings, users will need
to update the model definition by hand prior to using AIMET features.</p>
<p>For more information on model validator and preparer, refer to the corresponding sections in
<a class="reference internal" href="../api_docs/torch_quantization.html"><span class="doc">AIMET PyTorch Quantization APIs</span></a>.</p>
</dd>
<dt>Tensorflow:</dt><dd><p><a class="reference internal" href="../api_docs/tensorflow_model_guidelines.html"><span class="doc">TensorFlow Model Guidelines</span></a></p>
</dd>
</dl>
<p><strong>2. PTQ/AutoQuant</strong></p>
<p>The user can apply various PTQ techniques to the model to adjust model parameters and make the model more robust to
quantization. We recommend trying AutoQuant first, a PTQ feature which internally tries various other PTQ methods and
finds the best combination of methods to apply. Refer to the
AIMET Quantization Features section for more details on PTQ/AutoQuant.</p>
<p><strong>3. QAT</strong></p>
<p>If model accuracy is still not satisfactory after PTQ/AutoQuant, the user can use QAT to fine-tune the model. Refer to
the AIMET Quantization Features section for more details on QAT.</p>
<p><strong>4. Exporting models</strong></p>
<p>In order to bring the model onto the target, users will need two things:</p>
<ul class="simple">
<li><p>a model with updated weights</p></li>
<li><p>an encodings file containing quantization parameters associated with each quantization op</p></li>
</ul>
<p>AIMET QuantSim provides export functionality to generate both items. The exported model type will differ based on the ML
framework used:</p>
<ul class="simple">
<li><p>.onnx for PyTorch</p></li>
<li><p>meta/checkpoint for TensorFlow</p></li>
<li><p>.h5 and .pb for Keras</p></li>
</ul>
<p>Depending on which AIMET Quantization features were used, the user may need to take different steps to export the model
and encodings file. For example, calling AutoQuant will automatically export the model and encodings file as part of its
processing. If QAT is used, users will need to call .export() on the QuantSim object. If lower level PTQ techniques like
CLE are used, users will need to first create a QuantSim object from the modified model, and then call .export() on the
QuantSim object.</p>
<p><strong>5. Running on SNPE/QNN</strong></p>
<p>TODO: Include command for running model on target</p>
</div>
<div class="section" id="debugging-guidelines">
<h2>Debugging Guidelines<a class="headerlink" href="#debugging-guidelines" title="Permalink to this headline">¶</a></h2>
<p>Applying AIMET Quantization features may involve some trial and error in order to find the best optimizations to apply
on a particular model. We have included some debugging steps in the <a class="reference internal" href="quantization_feature_guidebook.html#ug-quant-guidebook"><span class="std std-ref">Quantization Guidebook</span></a>
that can be tried when quantization accuracy does not seem to improve right off the bat.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="model_compression.html" title="AIMET Model Compression"
             >next</a> |</li>
        <li class="right" >
          <a href="index.html" title="AI Model Efficiency Toolkit User Guide"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">AI Model Efficiency Toolkit Documentation: ver tf-torch-cpu_1.24.0</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Qualcomm Innovation Center, Inc..
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.1.1.
    </div>
  </body>
</html>